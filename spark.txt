# 运行原理

```text
spark-submit 
  -- class
  -- master
  -- deploy-mode 
  <application-jar>
  [appliccation-arguments]
  
  -- num-executors
  -- totoal-executor-cores
  -- executor-memory
    

Application --------------------- Cluster Manager --------------------- Worker Node
Driver                            Standalone                            Executor
SparkContext                      Yarn                                  Task

DAGScheduler 
TaskScheduler
Job -> Stage -> Task
```


## DAGScheduler 
RDD - partitions = tasks
      - expected: k * (#of executors * #of cores for each executor)
      - map
          - HDFS -> sc.textFile(path, minPartitions) -> #of partition = max(#of blocks, minPartition)
      - reduce: reduce阶段，shuffle操作，聚合以后rdd的partition数和具体操作有关 
          - reduce的task个数 = stage第一个rdd(shuffledRDD)partition数，取决于Partitioner
          - 聚合操作
            - 默认：spark.defalut.parallelism, spark.sql.shuffle.partitions
            - partitioner: reduceByKey(partitioner, func), reduceByKey(func, numPartitions)
          - join
            - 

      - shuffle
        - partitionr - HashPartitioner(default) 数据倾斜
                     - RangePartioner(抽样，均衡分割点）
        - 
          
                      
      
    - When we apply a transformation on a RDD, the transformation is applied to each of its partition
    - repartition() shuffles the data between the executors and divides the data into number of partitions, involves network traffic

DAG - pipelining transformations converts the logical DAG into physical execution plan of several stages

stage - task set - 一系列并发执行的tasks
      - combination of transformations which does not cause any shuffling
      - pipelining as many narrow transformations (eg: map, filter etc) as possible. -- parallize
      - shuffle

## TaskScheduler - task级别的调度
                 - executor - partition个数
                 -



## Executor - 


e.g. 计算资源：10 executors * 2 core(threads) for each executor -> 20 tasks(同一时刻可以并行20 tasks) 
     任务：rdd of 100 partitions -> 100 tasks -> 计算这个rdd需要5个轮次



broadcast variable



## MapReduce








# 接口

## DataFrame DataSet

## transformation - map, filter, reduceByKey, join
   action - count, collect, take, foreach
   
   





# 应用
## topK


            - 小文件：sc.textFile(InputSplit), InputSplit把若干Block合并成一个分片，不能跨越文件
  
